## 要用中文的 
<pre>
from transformers import pipeline

# Load a pre-trained Chinese text generation model
generator = pipeline('text-generation', model='uer/gpt2-chinese-cluecorpussmall')

# Generate some text
result = generator("幾週前, 我們國中同學聚會", max_length=30, num_return_sequences=1)

# Print the generated text
print(result)




  
  # chcp 65001
(venv) C:\tts\py311>python gpt2_zh.py
C:\tts\py311\venv\Lib\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': '幾週前, 我們國中同學聚會木号�'}]

(venv) C:\tts\py311>python gpt2_zh.py
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 577/577 [00:00<?, ?B/s]
pytorch_model.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 421M/421M [00:20<00:00, 20.9MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 217/217 [00:00<?, ?B/s]
vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 110k/110k [00:00<00:00, 650kB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<?, ?B/s]
C:\tts\py311\venv\Lib\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
[{'generated_text': '幾週前, 我們國中同學聚會 飯 後 聚 會 都 在 樓 上 的 麥 悅 ， 他 說 到 這'}]
</pre>



<pre>
(venv) C:\tts\py311>type gpt2.py
from transformers import pipeline

# Load a pre-trained text generation model
generator = pipeline('text-generation', model='gpt2')

# Generate some text
result = generator("Once upon a time", max_length=30, num_return_sequences=1)

# Print the generated text
print(result)

(venv) C:\tts\py311>python gpt2.py
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 665/665 [00:00<?, ?B/s]
model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 548M/548M [00:08<00:00, 65.1MB/s]
generation_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<?, ?B/s]
tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 26.0/26.0 [00:00<?, ?B/s]
vocab.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 1.53MB/s]
merges.txt: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 896kB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 1.55MB/s]
C:\tts\py311\venv\Lib\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': 'Once upon a time, the last thing the great city had was a bad time. It had failed once in a lifetime and was only recently that someone'}]

(venv) C:\tts\py311>
  
</pre>
